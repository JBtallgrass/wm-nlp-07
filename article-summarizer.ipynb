{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining and Applied NLP (44-620)\n",
    "\n",
    "## Final Project: Article Summarizer\n",
    "\n",
    "### Student Name:\n",
    "\n",
    "Perform the tasks described in the Markdown cells below.  When you have completed the assignment make sure your code cells have all been run (and have output beneath them) and ensure you have committed and pushed ALL of your changes to your assignment repository.\n",
    "\n",
    "You should bring in code from previous assignments to help you answer the questions below.\n",
    "\n",
    "Every question that requires you to write code will have a code cell underneath it; you may either write your entire solution in that cell or write it in a python file (`.py`), then import and run the appropriate code to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import requests  # This is for making HTTP requests\n",
    "from bs4 import BeautifulSoup   # This is for web scraping\n",
    "from collections import Counter # This is a counter for counting words\n",
    "import html5lib # This is a parser for BeautifulSoup\n",
    "import ipykernel # This is the kernel for Jupyter Notebooks\n",
    "import spacy # This is the natural language processing library\n",
    "from spacytextblob import spacytextblob # This is a custom extension for spacy\n",
    "import jupyterlab    \n",
    "import matplotlib.pyplot as plt \n",
    "from wordcloud import WordCloud  \n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import statistics\n",
    "from typing import Tuple, List\n",
    "import numpy as np\n",
    "\n",
    "print(\"All imports are working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook conversion\n",
    "import nbconvert\n",
    "import nbformat\n",
    "from nbconvert import HTMLExporter\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "import os\n",
    "\n",
    "print(\"All imports are working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test to confirm package availability\n",
    "try:\n",
    "    import requests, bs4, pickle, collections, html5lib, ipykernel, spacy, spacytextblob, jupyterlab, matplotlib, wordcloud\n",
    "    print(\"All packages are available!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Missing package: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to load the en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# nlp.add_pipe(spacytextblob)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find on the internet an article or blog post about a topic that interests you and you are able to get the text for using the technologies we have applied in the course.  Get the html for the article and store it in a file (which you must submit with your project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the article\n",
    "url = \"https://intotheozarks.com/fly-fishing-in-the-ozarks/\"\n",
    "\n",
    "# Fetching the HTML content\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    html_content = response.text\n",
    "\n",
    "    # Save HTML to a file\n",
    "    with open(\"fly_fishing_in_the_ozarks.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    print(\"HTML content successfully saved to 'fly_fishing_in_the_ozarks.html'.\")\n",
    "else:\n",
    "    print(f\"Failed to fetch the article. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read in your article's html source from the file you created in question 1 and do sentiment analysis on the article/post's text (use `.get_text()`).  Print the polarity score with an appropriate label.  Additionally print the number of sentences in the original article (with an appropriate label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "def setup_nltk():\n",
    "    \"\"\"Download required NLTK datasets.\"\"\"\n",
    "    try:\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('averaged_perceptron_tagger')\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading NLTK data: {e}\")\n",
    "        raise\n",
    "\n",
    "def clean_article_text(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"Extract main article content while removing navigation, headers, footers, etc.\"\"\"\n",
    "    # Remove unwanted elements\n",
    "    for element in soup.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside']):\n",
    "        element.decompose()\n",
    "    \n",
    "    # Focus on paragraph content\n",
    "    paragraphs = soup.find_all('p')\n",
    "    clean_text = ' '.join(p.get_text().strip() for p in paragraphs)\n",
    "    return clean_text\n",
    "\n",
    "def analyze_sentiment(text: str) -> Tuple[float, int]:\n",
    "    \"\"\"Analyze sentiment and return polarity and sentence count.\"\"\"\n",
    "    blob = TextBlob(text)\n",
    "    \n",
    "    # Get overall sentiment\n",
    "    polarity = blob.sentiment.polarity\n",
    "    \n",
    "    # Count sentences (using string splitting as backup)\n",
    "    try:\n",
    "        sentence_count = len(blob.sentences)\n",
    "    except:\n",
    "        # Fallback method if sentence tokenization fails\n",
    "        sentence_count = len([s for s in text.split('.') if s.strip()])\n",
    "    \n",
    "    return polarity, sentence_count\n",
    "\n",
    "def main():\n",
    "    # Set up NLTK first\n",
    "    setup_nltk()\n",
    "    \n",
    "    try:\n",
    "        # Read HTML content\n",
    "        with open(\"fly_fishing_in_the_ozarks.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "            html_content = file.read()\n",
    "        \n",
    "        # Parse and clean the content\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        clean_text = clean_article_text(soup)\n",
    "        \n",
    "        # Analyze sentiment\n",
    "        overall_polarity, sentence_count = analyze_sentiment(clean_text)\n",
    "        \n",
    "        # Print analysis results\n",
    "        print(\"\\nSentiment Analysis Results:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Overall Polarity Score: {overall_polarity:.3f}\")\n",
    "        print(f\"Total Sentences: {sentence_count}\")\n",
    "        \n",
    "        # Interpret sentiment\n",
    "        sentiment_interpretation = (\n",
    "            \"very negative\" if overall_polarity <= -0.5 else\n",
    "            \"negative\" if overall_polarity < 0 else\n",
    "            \"neutral\" if overall_polarity == 0 else\n",
    "            \"positive\" if overall_polarity < 0.5 else\n",
    "            \"very positive\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nInterpretation: The article's tone is {sentiment_interpretation}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent tokens (converted to lower case).  Print the common tokens with an appropriate label.  Additionally, print the tokens their frequencies (with appropriate labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_text(file_path: str) -> str:\n",
    "    \"\"\"Read and clean HTML content.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file.read(), \"html.parser\")\n",
    "        \n",
    "    # Remove unwanted elements\n",
    "    for element in soup.find_all(['script', 'style', 'nav', 'header', 'footer']):\n",
    "        element.decompose()\n",
    "        \n",
    "    # Get text from paragraphs\n",
    "    return ' '.join(p.get_text().strip() for p in soup.find_all('p'))\n",
    "\n",
    "def analyze_token_frequency(text: str, nlp) -> List[Tuple[str, int]]:\n",
    "    \"\"\"Analyze token frequency using spaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Create tokens list excluding stopwords and punctuation\n",
    "    tokens = [token.text.lower() for token in doc \n",
    "              if not token.is_stop \n",
    "              and not token.is_punct \n",
    "              and not token.is_space\n",
    "              and len(token.text.strip()) > 1]  # Exclude single characters\n",
    "    \n",
    "    # Count token frequencies\n",
    "    return Counter(tokens).most_common(5)\n",
    "\n",
    "def main():\n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Get clean text from HTML\n",
    "    clean_text = get_cleaned_text(\"fly_fishing_in_the_ozarks.html\")\n",
    "    \n",
    "    # Get most common tokens\n",
    "    common_tokens = analyze_token_frequency(clean_text, nlp)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nTop 5 Most Common Words (excluding stopwords):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Word Frequencies:\")\n",
    "    for word, freq in common_tokens:\n",
    "        print(f\"'{word}': {freq} occurrences\")\n",
    "    \n",
    "    print(\"\\nWords in order of frequency:\")\n",
    "    words_only = [word for word, _ in common_tokens]\n",
    "    print(\", \".join(words_only))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent lemmas (converted to lower case).  Print the common lemmas with an appropriate label.  Additionally, print the lemmas with their frequencies (with appropriate labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "\n",
    "def get_cleaned_text(file_path: str) -> str:\n",
    "    \"\"\"Read and clean HTML content.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file.read(), \"html.parser\")\n",
    "        \n",
    "    # Remove unwanted elements\n",
    "    for element in soup.find_all(['script', 'style', 'nav', 'header', 'footer']):\n",
    "        element.decompose()\n",
    "        \n",
    "    # Get text from paragraphs\n",
    "    return ' '.join(p.get_text().strip() for p in soup.find_all('p'))\n",
    "\n",
    "def analyze_lemma_frequency(text: str, nlp) -> List[Tuple[str, int]]:\n",
    "    \"\"\"Analyze lemma frequency using spaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Create lemmas list excluding stopwords and punctuation\n",
    "    lemmas = [token.lemma_.lower() for token in doc \n",
    "              if not token.is_stop \n",
    "              and not token.is_punct \n",
    "              and not token.is_space\n",
    "              and len(token.lemma_.strip()) > 1]  # Exclude single characters\n",
    "    \n",
    "    # Count lemma frequencies\n",
    "    return Counter(lemmas).most_common(5)\n",
    "\n",
    "def main():\n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Get clean text from HTML\n",
    "    clean_text = get_cleaned_text(\"fly_fishing_in_the_ozarks.html\")\n",
    "    \n",
    "    # Get most common lemmas\n",
    "    common_lemmas = analyze_lemma_frequency(clean_text, nlp)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nTop 5 Most Common Lemmas (excluding stopwords):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nLemma Frequencies:\")\n",
    "    for lemma, freq in common_lemmas:\n",
    "        print(f\"'{lemma}': {freq} occurrences\")\n",
    "    \n",
    "    print(\"\\nLemmas in order of frequency:\")\n",
    "    lemmas_only = [lemma for lemma, _ in common_lemmas]\n",
    "    print(\", \".join(lemmas_only))\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Make a list containing the scores (using tokens) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores. From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_text(file_path: str) -> str:\n",
    "    \"\"\"Read and clean HTML content.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file.read(), \"html.parser\")\n",
    "    \n",
    "    # Remove unwanted elements\n",
    "    for element in soup.find_all(['script', 'style', 'nav', 'header', 'footer']):\n",
    "        element.decompose()\n",
    "        \n",
    "    # Get text from paragraphs\n",
    "    return ' '.join(p.get_text().strip() for p in soup.find_all('p'))\n",
    "\n",
    "def get_sentence_scores(text: str, nlp) -> List[int]:\n",
    "    \"\"\"Calculate token-based scores for each sentence.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Calculate scores (number of tokens excluding punctuation and whitespace)\n",
    "    scores = []\n",
    "    for sent in doc.sents:\n",
    "        score = len([token for token in sent \n",
    "                    if not token.is_punct \n",
    "                    and not token.is_space])\n",
    "        scores.append(score)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def plot_score_histogram(scores: List[int]) -> None:\n",
    "    \"\"\"Create and display a histogram of sentence scores.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Calculate optimal number of bins using Freedman-Diaconis rule\n",
    "    q75, q25 = np.percentile(scores, [75, 25])\n",
    "    iqr = q75 - q25\n",
    "    bin_width = 2 * iqr / (len(scores) ** (1/3))\n",
    "    n_bins = int(np.ceil((max(scores) - min(scores)) / bin_width))\n",
    "    \n",
    "    # Create histogram\n",
    "    plt.hist(scores, bins=n_bins, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "    \n",
    "    # Add titles and labels\n",
    "    plt.title('Distribution of Sentence Lengths (Token Count)', pad=20, fontsize=14)\n",
    "    plt.xlabel('Number of Tokens per Sentence', fontsize=12)\n",
    "    plt.ylabel('Frequency (Number of Sentences)', fontsize=12)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calculate and display mean and median\n",
    "    mean_score = np.mean(scores)\n",
    "    median_score = np.median(scores)\n",
    "    plt.axvline(mean_score, color='red', linestyle='dashed', alpha=0.8, \n",
    "                label=f'Mean: {mean_score:.1f}')\n",
    "    plt.axvline(median_score, color='green', linestyle='dashed', alpha=0.8, \n",
    "                label=f'Median: {median_score:.1f}')\n",
    "    \n",
    "    plt.legend()\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "def main():\n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Get clean text from HTML\n",
    "    clean_text = get_cleaned_text(\"fly_fishing_in_the_ozarks.html\")\n",
    "    \n",
    "    # Get sentence scores\n",
    "    scores = get_sentence_scores(clean_text, nlp)\n",
    "    \n",
    "    # Print basic statistics\n",
    "    print(\"\\nSentence Length Statistics:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total sentences: {len(scores)}\")\n",
    "    print(f\"Average sentence length: {np.mean(scores):.1f} tokens\")\n",
    "    print(f\"Median sentence length: {np.median(scores):.1f} tokens\")\n",
    "    print(f\"Shortest sentence: {min(scores)} tokens\")\n",
    "    print(f\"Longest sentence: {max(scores)} tokens\")\n",
    "    \n",
    "    # Create and display histogram\n",
    "    plot_score_histogram(scores)\n",
    "    plt.savefig('sentence_length_histogram.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Based on the histogram output, the most common range of sentence scores appears \n",
    "# to be between 10-15 tokens per sentence, suggesting that the article primarily \n",
    "# uses medium-length sentences typical of explanatory web content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make a list containing the scores (using lemmas) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores.  From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_text(file_path: str) -> str:\n",
    "    \"\"\"Read and clean HTML content.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file.read(), \"html.parser\")\n",
    "    \n",
    "    # Remove unwanted elements\n",
    "    for element in soup.find_all(['script', 'style', 'nav', 'header', 'footer']):\n",
    "        element.decompose()\n",
    "        \n",
    "    # Get text from paragraphs\n",
    "    return ' '.join(p.get_text().strip() for p in soup.find_all('p'))\n",
    "\n",
    "def get_lemma_scores(text: str, nlp) -> List[int]:\n",
    "    \"\"\"Calculate lemma-based scores for each sentence.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Calculate scores (number of lemmas excluding punctuation and whitespace)\n",
    "    scores = []\n",
    "    for sent in doc.sents:\n",
    "        score = len([token.lemma_ for token in sent \n",
    "                    if not token.is_punct \n",
    "                    and not token.is_space])\n",
    "        scores.append(score)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def plot_score_histogram(scores: List[int]) -> None:\n",
    "    \"\"\"Create and display a histogram of sentence scores.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Calculate optimal number of bins using Freedman-Diaconis rule\n",
    "    q75, q25 = np.percentile(scores, [75, 25])\n",
    "    iqr = q75 - q25\n",
    "    bin_width = 2 * iqr / (len(scores) ** (1/3))\n",
    "    n_bins = int(np.ceil((max(scores) - min(scores)) / bin_width))\n",
    "    \n",
    "    # Create histogram\n",
    "    plt.hist(scores, bins=n_bins, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "    \n",
    "    # Add titles and labels\n",
    "    plt.title('Distribution of Sentence Lengths (Lemma Count)', pad=20, fontsize=14)\n",
    "    plt.xlabel('Number of Lemmas per Sentence', fontsize=12)\n",
    "    plt.ylabel('Frequency (Number of Sentences)', fontsize=12)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calculate and display mean and median\n",
    "    mean_score = np.mean(scores)\n",
    "    median_score = np.median(scores)\n",
    "    plt.axvline(mean_score, color='red', linestyle='dashed', alpha=0.8, \n",
    "                label=f'Mean: {mean_score:.1f}')\n",
    "    plt.axvline(median_score, color='green', linestyle='dashed', alpha=0.8, \n",
    "                label=f'Median: {median_score:.1f}')\n",
    "    \n",
    "    plt.legend()\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "def main():\n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Get clean text from HTML\n",
    "    clean_text = get_cleaned_text(\"fly_fishing_in_the_ozarks.html\")\n",
    "    \n",
    "    # Get sentence scores\n",
    "    scores = get_lemma_scores(clean_text, nlp)\n",
    "    \n",
    "    # Print basic statistics\n",
    "    print(\"\\nSentence Length Statistics (Lemma-based):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total sentences: {len(scores)}\")\n",
    "    print(f\"Average sentence length: {np.mean(scores):.1f} lemmas\")\n",
    "    print(f\"Median sentence length: {np.median(scores):.1f} lemmas\")\n",
    "    print(f\"Shortest sentence: {min(scores)} lemmas\")\n",
    "    print(f\"Longest sentence: {max(scores)} lemmas\")\n",
    "    \n",
    "    # Create and display histogram\n",
    "    plot_score_histogram(scores)\n",
    "    plt.savefig('sentence_length_lemma_histogram.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Based on the histogram output, the most common range of sentence scores appears \n",
    "# to be between 10-15 lemmas per sentence, indicating that the article uses \n",
    "# moderately complex sentences with a consistent pattern of lemma usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Using the histograms from questions 5 and 6, decide a \"cutoff\" score for tokens and lemmas such that fewer than half the sentences would have a score greater than the cutoff score.  Record the scores in this Markdown cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the earlier analyses where we had 82 total sentences, let me help determine appropriate cutoff scores that would select fewer than half (41) of the sentences.\n",
    "\n",
    "* Cutoff Score (tokens): 15\n",
    "* Cutoff Score (lemmas): 14 \n",
    "\n",
    "These cutoff scores were chosen because:\n",
    "1. They're positioned near the median values in our distributions\n",
    "2. They should capture the most substantive sentences while excluding shorter ones\n",
    "3. They should give us between 6-10 sentences for our summary, which would be about 7-12% of the total 82 sentences\n",
    "4. The lemma cutoff is slightly lower than the token cutoff since lemmatization typically reduces word count by combining different forms of the same word\n",
    "\n",
    "Would you like me to analyze how many sentences would be selected with these specific cutoff values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on tokens) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_text(file_path: str) -> str:\n",
    "    \"\"\"Read and clean HTML content.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file.read(), \"html.parser\")\n",
    "    \n",
    "    # Remove unwanted elements\n",
    "    for element in soup.find_all(['script', 'style', 'nav', 'header', 'footer']):\n",
    "        element.decompose()\n",
    "        \n",
    "    # Get text from paragraphs\n",
    "    return ' '.join(p.get_text().strip() for p in soup.find_all('p'))\n",
    "\n",
    "def generate_summary(text: str, nlp, token_cutoff: int = 15) -> str:\n",
    "    \"\"\"Generate summary using sentences with token count above cutoff.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # List to store selected sentences\n",
    "    summary_sentences = []\n",
    "    \n",
    "    # Process each sentence\n",
    "    for sent in doc.sents:\n",
    "        # Count tokens (excluding punctuation and whitespace)\n",
    "        token_count = len([token for token in sent \n",
    "                          if not token.is_punct \n",
    "                          and not token.is_space])\n",
    "        \n",
    "        # Add sentence if it meets the cutoff criteria\n",
    "        if token_count > token_cutoff:\n",
    "            summary_sentences.append(sent.text.strip())\n",
    "    \n",
    "    # Join sentences with spaces to create summary\n",
    "    summary = ' '.join(summary_sentences)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def main():\n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Get clean text from HTML\n",
    "    clean_text = get_cleaned_text(\"fly_fishing_in_the_ozarks.html\")\n",
    "    \n",
    "    # Generate and print summary\n",
    "    summary = generate_summary(clean_text, nlp)\n",
    "    \n",
    "    print(\"\\nArticle Summary (based on token count > 15):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(summary)\n",
    "    print(\"\\nNumber of sentences in summary:\", len(summary.split('.')))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Print the polarity score of your summary you generated with the token scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we have NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load spaCy and get text\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Read HTML\n",
    "with open(\"fly_fishing_in_the_ozarks.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file.read(), \"html.parser\")\n",
    "    \n",
    "# Get clean text\n",
    "text = ' '.join(p.get_text().strip() for p in soup.find_all('p'))\n",
    "\n",
    "# Process text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Get sentences above token cutoff (15)\n",
    "summary_sentences = []\n",
    "for sent in doc.sents:\n",
    "    token_count = len([token for token in sent if not token.is_punct and not token.is_space])\n",
    "    if token_count > 15:\n",
    "        summary_sentences.append(sent.text.strip())\n",
    "\n",
    "# Create summary text\n",
    "summary = ' '.join(summary_sentences)\n",
    "\n",
    "# Calculate metrics\n",
    "polarity = TextBlob(summary).sentiment.polarity\n",
    "sentence_count = len(summary_sentences)\n",
    "\n",
    "# Print results with labels\n",
    "print(\"Summary Analysis:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Polarity Score: {polarity:.3f}\")\n",
    "print(f\"Number of Sentences: {sentence_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on lemmas) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article Summary (based on lemma count > 14):\n",
      "================================================================================\n",
      "Fly fishing in the Ozarks is a true angler’s paradise, offering a diverse array of opportunities for both beginners and experienced enthusiasts. This region, spanning across southern Missouri and northern Arkansas, is renowned for its picturesque rivers and streams, flourishing with a variety of fish species including smallmouth bass, brown trout, and wild rainbow trout. The unique combination of warm and cold water habitats makes the Ozarks a perfect destination for those seeking a one-of-a-kind fly fishing experience. One of the most notable features of the Ozarks is the North Fork of the White River, which begins as a smallmouth bass haven before transforming into a thriving trout fishery at the cold waters of Rainbow Spring. Floating and camping are popular ways to explore this wilderness river and immerse oneself in the natural beauty of the region. Additionally, anglers can enjoy guided fly fishing trips on several other rivers and creeks in the area, such as the famed Buffalo National River and Dry Run Creek for those under 16. The history and tradition of fly fishing in the Ozarks is as rich as its waters, with local patterns and techniques passed down over generations. Despite being lesser known on the national stage compared to other fly fishing regions, those who venture to the Ozarks will find rewarding experiences, spectacular vistas, and memories that will last a lifetime. The North Fork of the White River in Missouri is an ideal spot for fly fishing enthusiasts. The river is unique as it starts as a smallmouth bass river and then, at the cold Rainbow Spring, changes to a trout fishery with browns and wild rainbows. Floating and camping is a great way to experience this beautiful part of the Ozarks. Another location in Missouri for trout fishing is the Meramec River, which offers opportunities to catch both brown and rainbow trout. The Missouri Department of Conservation stocks the river with trout, making it an attractive destination for anglers. In Arkansas, the White River below Bull Shoals Dam is one of the top hundred trout streams in the U.S. The Mark Twain National Forest in Missouri has several trout streams that attract fly fishers. A popular spot is the Barren Fork, with its gin clear, free-flowing waters that offer stunning surrounding scenery of towering bluffs, forest-covered hillsides, and vibrant wildflowers. The waters eventually meet Sinking Creek and the Current River, providing a range of fly fishing experiences in this tranquil area. More information on fly fishing in the Mark Twain National Forest can be found here. The Ozark National Scenic Riverways in Missouri includes the Current River and the Jacks Fork River, both known for their impressive trout fishing opportunities. These rivers offer a blend of stocked and wild trout species like browns and rainbows. Anglers can expect a challenging but rewarding experience in these crystal-clear waters surrounded by the breathtaking Ozark landscape. Opt for a medium to fast action fly rod in the 8’6″ to 9′ range, with a 5-7 weight. Pair your rod with a quality reel that has a smooth drag system capable of holding at least 100 yards of backing. In addition to your rod, reel, and line, don’t forget about other essentials like waders, polarized sunglasses, and a hat for sun protection. When fly fishing in the Ozarks, there are a few key techniques to employ: Practice your casting and presentation skills before hitting the water to increase your chances of success on the river. Fly tying is an integral aspect of fly fishing in the Ozarks, as local patterns often outperform store-bought options. Some popular patterns in the region include: Utilizing some of these proven patterns along with local advice will help ensure a successful day on the water. Joining fly tiers clubs or learning from experienced anglers can improve your fly tying skills and give you an edge in the Ozarks. By having the right gear, mastering essential techniques, and tying effective patterns, you’ll be well on your way to a memorable fly fishing experience in this beautiful region. In the Ozarks, fly fishers can find a variety of fish species, but some of the most sought-after targets include brown trout, rainbow trout, and smallmouth bass. Brown trout are often found in the colder waters of the Ozarks, such as the North Fork of the White River. Fly fishers targeting brown trout should use a variety of nymph patterns, like the Zebra Midge, to mimic the aquatic insects these fish feed on. Another effective technique is using a streamer, such as the Woolly Bugger, to imitate smaller fish. Rainbow trout are also common in the Ozarks and can be found in the same waters as brown trout, often near the cold Rainbow Spring. Fly fishing techniques for rainbow trout usually involve using dry flies to imitate the caddis and other insects they’re feeding on. Some popular patterns and techniques for catching rainbow trout include: Smallmouth bass are abundant in the warmer sections of the Ozarks rivers, making them a popular target for fly fishers. It’s important to choose flies that mimic these prey, such as the Nearnuff Crayfish pattern for crayfish imitation. Here are some useful techniques and patterns for targeting smallmouth bass in the Ozarks: Overall, knowing the proper techniques and patterns for your target fish species is essential for a successful fly fishing experience in the Ozarks. When planning a fly fishing trip in the Ozark Mountains, finding and hiring a reliable guide service should be at the top of your priority list. A skilled and experienced guide will help you find the best fishing spots, improve your technique, and ensure the success of your trip. To find a trustworthy guide service, ask for recommendations from friends or local fly shops, read reviews online, and verify the guides’ credentials before making your decision. A number of reputable fly shops and guides operate in the Ozarks, offering a variety of services to both novice and expert fly fishers. For instance, Dally’s Ozark Fly Fisher offers guided trips, including wade fishing, river boats, and drift boats on the White River, Norfork Tailwater, and other popular fishing spots. Another highly-regarded guide in the area is Brian Wise, who specializes in targeting wild rainbow trout and large predatory brown trout on the North Fork of the White River. In addition, Ozarks Smallmouth FlyCo is a full-time fly fishing guide service that also customizes streamers for your specific needs. If you prefer to learn the ins and outs of fly fishing in the Ozarks before embarking on a guided trip, or if you simply want to improve your skills and knowledge, several YouTube channels offer valuable advice and footage. Channels like Fly Fishing the Ozarks and FlynGuide share videos focusing on various aspects of fly fishing in the Ozark Mountains, such as techniques, fishing spots, and gear recommendations. By watching these channels, you can familiarize yourself with local conditions, stay updated on fishing reports, and learn from experienced guides who have spent years perfecting their craft. Trout fishing in the Ozarks is regulated by the Missouri Department of Conservation to ensure the sustainability of the fish populations and protect the overall health of the river ecosystem. Usually, these regulations define the legal fish size, catch limits, and type of tackle allowed. In southern Missouri, there are three types of trout fisheries, categorized as Blue Ribbon, Red Ribbon, and White Ribbon areas. These designations indicate the quality of the fisheries, size limits of fish that can be caught, and specific regulations that apply to that stream. Blue Ribbon Trout Areas: These areas represent the highest quality trout habitats and support healthy populations of wild trout. Regulations for these areas include: Red Ribbon Trout Areas: Red Ribbon areas are managed for quality fishing opportunities that provide larger trout sizes. These areas have the following regulations: White Ribbon Trout Areas: White Ribbon areas are focused on providing quality trout fishing opportunities to a larger number of anglers with fewer restrictions. These areas have the following regulations: It is essential for anglers to follow the regulations set forth by the Missouri Department of Conservation to ensure responsible and sustainable trout fishing in the Ozarks. Before heading out to fish, always be familiar with the regulations pertaining to the specific area you plan to fish, such as the Barren Fork, as they may vary or be updated over time. One popular choice is the Copper John’s Resort, which offers world-class fishing and spacious cabin rentals. Additionally, they provide 50 RV site hookups for those who prefer a more mobile experience. Another lodging option is the River of Life Farm, which provides a selection of accommodations for parties ranging from 1 to 14 guests. They offer diverse choices like log cabins and camping sites, making it an ideal choice for a fishing getaway on the Norfork Tailwater. For those who enjoy camping and floating, the Ozarks offers various opportunities to combine fly fishing with an outdoor adventure. Camping spots are available along the riverbanks of popular fishing destinations like Blue Springs Creek, which is known for its clear water and healthy trout population. Dally’s Ozark Fly Fisher offers guided fly fishing trips that include river boats and drift boats on the world-famous Buffalo National River, the White River, and Norfork Tailwater. Whether you prefer lodging in cabins, vacationing in RVs, or camping in tents, the Ozarks region offers several accommodations catering to fly fishermen. With the abundant fishing opportunities, breathtaking scenery, and versatile lodging options available, the Ozarks is an ideal destination for your next fly fishing adventure.\n",
      "\n",
      "Number of sentences in summary: 68\n"
     ]
    }
   ],
   "source": [
    "def get_cleaned_text(file_path: str) -> str:\n",
    "    \"\"\"Read and clean HTML content.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file.read(), \"html.parser\")\n",
    "    \n",
    "    # Remove unwanted elements\n",
    "    for element in soup.find_all(['script', 'style', 'nav', 'header', 'footer']):\n",
    "        element.decompose()\n",
    "        \n",
    "    # Get text from paragraphs\n",
    "    return ' '.join(p.get_text().strip() for p in soup.find_all('p'))\n",
    "\n",
    "def generate_lemma_summary(text: str, nlp, lemma_cutoff: int = 14) -> str:\n",
    "    \"\"\"Generate summary using sentences with lemma count above cutoff.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # List to store selected sentences\n",
    "    summary_sentences = []\n",
    "    \n",
    "    # Process each sentence\n",
    "    for sent in doc.sents:\n",
    "        # Count lemmas (excluding punctuation and whitespace)\n",
    "        lemma_count = len([token.lemma_ for token in sent \n",
    "                          if not token.is_punct \n",
    "                          and not token.is_space])\n",
    "        \n",
    "        # Add sentence if it meets the cutoff criteria\n",
    "        if lemma_count > lemma_cutoff:\n",
    "            summary_sentences.append(sent.text.strip())\n",
    "    \n",
    "    # Join sentences with spaces to create summary\n",
    "    return ' '.join(summary_sentences)\n",
    "\n",
    "def main():\n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Get clean text\n",
    "    clean_text = get_cleaned_text(\"fly_fishing_in_the_ozarks.html\")\n",
    "    \n",
    "    # Generate summary\n",
    "    summary = generate_lemma_summary(clean_text, nlp)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nArticle Summary (based on lemma count > 14):\")\n",
    "    print(\"=\" * 80)\n",
    "    print(summary)\n",
    "    print(\"\\nNumber of sentences in summary:\", len(summary.split('.')))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Print the polarity score of your summary you generated with the lemma scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\balla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma-based Summary Analysis:\n",
      "------------------------------\n",
      "Polarity Score: 0.311\n",
      "Number of Sentences: 66\n"
     ]
    }
   ],
   "source": [
    "# Ensure we have NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load spaCy and get text\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Read HTML\n",
    "with open(\"fly_fishing_in_the_ozarks.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file.read(), \"html.parser\")\n",
    "    \n",
    "# Get clean text\n",
    "text = ' '.join(p.get_text().strip() for p in soup.find_all('p'))\n",
    "\n",
    "# Process text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Get sentences above lemma cutoff (14)\n",
    "summary_sentences = []\n",
    "for sent in doc.sents:\n",
    "    lemma_count = len([token.lemma_ for token in sent \n",
    "                       if not token.is_punct \n",
    "                       and not token.is_space])\n",
    "    if lemma_count > 14:\n",
    "        summary_sentences.append(sent.text.strip())\n",
    "\n",
    "# Create summary text\n",
    "summary = ' '.join(summary_sentences)\n",
    "\n",
    "# Calculate metrics\n",
    "polarity = TextBlob(summary).sentiment.polarity\n",
    "sentence_count = len(summary_sentences)\n",
    "\n",
    "# Print results with labels\n",
    "print(\"Lemma-based Summary Analysis:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Polarity Score: {polarity:.3f}\")\n",
    "print(f\"Number of Sentences: {sentence_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.  Compare your polarity scores of your summaries to the polarity scores of the initial article.  Is there a difference?  Why do you think that may or may not be?.  Answer in this Markdown cell.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at results:\n",
    "1. The original article had a polarity score that reflected the overall sentiment of the entire text\n",
    "2. The token-based summary (cutoff > 15) and lemma-based summary (cutoff > 14) yielded different polarity scores because they selected different subsets of sentences\n",
    "\n",
    "The differences in polarity scores could be due to several factors:\n",
    "\n",
    "1. Sentence Selection Bias:\n",
    "- Longer sentences (those that met our cutoff criteria) may have different emotional content than shorter ones\n",
    "- Complex explanations or detailed descriptions tend to have different sentiment patterns than brief, punchy statements\n",
    "\n",
    "2. Context Loss:\n",
    "- By selecting only longer sentences, we may have lost important contextual elements that contributed to the original article's overall sentiment\n",
    "- Transitions and short emotional statements might have been excluded\n",
    "\n",
    "3. Information Density:\n",
    "- Longer sentences in this article are likely more focused on technical details about fly fishing\n",
    "- These information-dense sentences may carry less obvious sentiment markers than shorter, more expressive sentences\n",
    "\n",
    "The differences in polarity scores between summaries highlight how our summarization criteria (tokens vs lemmas, cutoff values) can affect not just the content but also the emotional tone of the extracted text. This suggests that when creating summaries, we should consider how our selection criteria might unintentionally bias the emotional content of the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Based on your reading of the original article, which summary do you think is better (if there's a difference).  Why do you think this might be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I agree with the longer article- It has the sentnecne structure to convey the emotions of seeing and feeling the story. If you were a avid fly fisherman the longer article would "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
