{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining and Applied NLP (44-620)\n",
    "\n",
    "## Final Project: Article Summarizer\n",
    "\n",
    "### Student Name: Jason A. Ballard \n",
    "\n",
    "Perform the tasks described in the Markdown cells below.  When you have completed the assignment make sure your code cells have all been run (and have output beneath them) and ensure you have committed and pushed ALL of your changes to your assignment repository.\n",
    "\n",
    "You should bring in code from previous assignments to help you answer the questions below.\n",
    "\n",
    "Every question that requires you to write code will have a code cell underneath it; you may either write your entire solution in that cell or write it in a python file (`.py`), then import and run the appropriate code to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import requests  # This is for making HTTP requests\n",
    "from bs4 import BeautifulSoup   # This is for web scraping\n",
    "from collections import Counter # This is a counter for counting words\n",
    "import html5lib # This is a parser for BeautifulSoup\n",
    "import ipykernel # This is the kernel for Jupyter Notebooks\n",
    "import spacy # This is the natural language processing library\n",
    "from spacytextblob import spacytextblob # This is a custom extension for spacy\n",
    "import jupyterlab    \n",
    "import matplotlib.pyplot as plt \n",
    "from wordcloud import WordCloud  \n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import statistics\n",
    "from typing import Tuple, List\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "print(\"All imports are working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook conversion\n",
    "import nbconvert\n",
    "import nbformat\n",
    "from nbconvert import HTMLExporter\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "import os\n",
    "\n",
    "print(\"All imports are working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test to confirm package availability\n",
    "try:\n",
    "    import requests, bs4, pickle, collections, html5lib, ipykernel, spacy, spacytextblob, jupyterlab, matplotlib, wordcloud\n",
    "    print(\"All packages are available!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Missing package: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to load the en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# nlp.add_pipe(spacytextblob)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find on the internet an article or blog post about a topic that interests you and you are able to get the text for using the technologies we have applied in the course.  Get the html for the article and store it in a file (which you must submit with your project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the article\n",
    "url = \"https://www.gutenberg.org/cache/epub/18868/pg18868-images.html\"\n",
    "\n",
    "# Fetching the HTML content\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    html_content = response.text\n",
    "\n",
    "    # Save HTML to a file\n",
    "    with open(\"kitchener.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    print(\"HTML content successfully saved to 'kitchener.html'.\")\n",
    "else:\n",
    "    print(f\"Failed to fetch the article. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read in your article's html source from the file you created in question 1 and do sentiment analysis on the article/post's text (use `.get_text()`).  Print the polarity score with an appropriate label.  Additionally print the number of sentences in the original article (with an appropriate label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "def setup_nltk():\n",
    "    \"\"\"Download required NLTK datasets.\"\"\"\n",
    "    try:\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('averaged_perceptron_tagger')\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading NLTK data: {e}\")\n",
    "        raise\n",
    "\n",
    "def clean_article_text(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"Extract main article content while removing navigation, headers, footers, etc.\"\"\"\n",
    "    # Remove unwanted elements\n",
    "    for element in soup.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside']):\n",
    "        element.decompose()\n",
    "    \n",
    "    # Focus on paragraph content\n",
    "    paragraphs = soup.find_all('p')\n",
    "    clean_text = ' '.join(p.get_text().strip() for p in paragraphs)\n",
    "    return clean_text\n",
    "\n",
    "def analyze_sentiment(text: str) -> Tuple[float, int]:\n",
    "    \"\"\"Analyze sentiment and return polarity and sentence count.\"\"\"\n",
    "    blob = TextBlob(text)\n",
    "    \n",
    "    # Get overall sentiment\n",
    "    polarity = blob.sentiment.polarity\n",
    "    \n",
    "    # Count sentences (using string splitting as backup)\n",
    "    try:\n",
    "        sentence_count = len(blob.sentences)\n",
    "    except:\n",
    "        # Fallback method if sentence tokenization fails\n",
    "        sentence_count = len([s for s in text.split('.') if s.strip()])\n",
    "    \n",
    "    return polarity, sentence_count\n",
    "\n",
    "def main():\n",
    "    # Set up NLTK first\n",
    "    setup_nltk()\n",
    "    \n",
    "    try:\n",
    "        # Read HTML content\n",
    "        with open(\"kitchener.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "            html_content = file.read()\n",
    "        \n",
    "        # Parse and clean the content\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        clean_text = clean_article_text(soup)\n",
    "        \n",
    "        # Analyze sentiment\n",
    "        overall_polarity, sentence_count = analyze_sentiment(clean_text)\n",
    "        \n",
    "        # Print analysis results\n",
    "        print(\"\\nSentiment Analysis Results:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Overall Polarity Score: {overall_polarity:.3f}\")\n",
    "        print(f\"Total Sentences: {sentence_count}\")\n",
    "        \n",
    "        # Interpret sentiment\n",
    "        sentiment_interpretation = (\n",
    "            \"very negative\" if overall_polarity <= -0.5 else\n",
    "            \"negative\" if overall_polarity < 0 else\n",
    "            \"neutral\" if overall_polarity == 0 else\n",
    "            \"positive\" if overall_polarity < 0.5 else\n",
    "            \"very positive\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nInterpretation: The article's tone is {sentiment_interpretation}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent tokens (converted to lower case).  Print the common tokens with an appropriate label.  Additionally, print the tokens their frequencies (with appropriate labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_text(file_path: str) -> str:\n",
    "    \"\"\"Read and clean HTML content.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file.read(), \"html.parser\")\n",
    "        \n",
    "    # Remove unwanted elements\n",
    "    for element in soup.find_all(['script', 'style', 'nav', 'header', 'footer']):\n",
    "        element.decompose()\n",
    "        \n",
    "    # Get text from paragraphs\n",
    "    return ' '.join(p.get_text().strip() for p in soup.find_all('p'))\n",
    "\n",
    "def analyze_token_frequency(text: str, nlp) -> List[Tuple[str, int]]:\n",
    "    \"\"\"Analyze token frequency using spaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Create tokens list excluding stopwords and punctuation\n",
    "    tokens = [token.text.lower() for token in doc \n",
    "              if not token.is_stop \n",
    "              and not token.is_punct \n",
    "              and not token.is_space\n",
    "              and len(token.text.strip()) > 1]  # Exclude single characters\n",
    "    \n",
    "    # Count token frequencies\n",
    "    return Counter(tokens).most_common(5)\n",
    "\n",
    "def main():\n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Get clean text from HTML\n",
    "    clean_text = get_cleaned_text(\"kitchener.html\")\n",
    "    \n",
    "    # Get most common tokens\n",
    "    common_tokens = analyze_token_frequency(clean_text, nlp)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nTop 5 Most Common Words (excluding stopwords):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Word Frequencies:\")\n",
    "    for word, freq in common_tokens:\n",
    "        print(f\"'{word}': {freq} occurrences\")\n",
    "    \n",
    "    print(\"\\nWords in order of frequency:\")\n",
    "    words_only = [word for word, _ in common_tokens]\n",
    "    print(\", \".join(words_only))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent lemmas (converted to lower case).  Print the common lemmas with an appropriate label.  Additionally, print the lemmas with their frequencies (with appropriate labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_text(file_path: str) -> str:\n",
    "    \"\"\"Read and clean HTML content.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file.read(), \"html.parser\")\n",
    "        \n",
    "    # Remove unwanted elements\n",
    "    for element in soup.find_all(['script', 'style', 'nav', 'header', 'footer']):\n",
    "        element.decompose()\n",
    "        \n",
    "    # Get text from paragraphs\n",
    "    return ' '.join(p.get_text().strip() for p in soup.find_all('p'))\n",
    "\n",
    "def analyze_lemma_frequency(text: str, nlp) -> List[Tuple[str, int]]:\n",
    "    \"\"\"Analyze lemma frequency using spaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Create lemmas list excluding stopwords and punctuation\n",
    "    lemmas = [token.lemma_.lower() for token in doc \n",
    "              if not token.is_stop \n",
    "              and not token.is_punct \n",
    "              and not token.is_space\n",
    "              and len(token.lemma_.strip()) > 1]  # Exclude single characters\n",
    "    \n",
    "    # Count lemma frequencies\n",
    "    return Counter(lemmas).most_common(5)\n",
    "\n",
    "def main():\n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Get clean text from HTML\n",
    "    clean_text = get_cleaned_text(\"kitchener.html\")\n",
    "    \n",
    "    # Get most common lemmas\n",
    "    common_lemmas = analyze_lemma_frequency(clean_text, nlp)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nTop 5 Most Common Lemmas (excluding stopwords):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nLemma Frequencies:\")\n",
    "    for lemma, freq in common_lemmas:\n",
    "        print(f\"'{lemma}': {freq} occurrences\")\n",
    "    \n",
    "    print(\"\\nLemmas in order of frequency:\")\n",
    "    lemmas_only = [lemma for lemma, _ in common_lemmas]\n",
    "    print(\", \".join(lemmas_only))\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Make a list containing the scores (using tokens) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores. From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_text(file_path: str) -> str:\n",
    "    \"\"\"Read and clean HTML content.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file.read(), \"html.parser\")\n",
    "    \n",
    "    # Remove unwanted elements\n",
    "    for element in soup.find_all(['script', 'style', 'nav', 'header', 'footer']):\n",
    "        element.decompose()\n",
    "        \n",
    "    # Get text from paragraphs\n",
    "    return ' '.join(p.get_text().strip() for p in soup.find_all('p'))\n",
    "\n",
    "def get_sentence_scores(text: str, nlp) -> List[int]:\n",
    "    \"\"\"Calculate token-based scores for each sentence.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Calculate scores (number of tokens excluding punctuation and whitespace)\n",
    "    scores = []\n",
    "    for sent in doc.sents:\n",
    "        score = len([token for token in sent \n",
    "                    if not token.is_punct \n",
    "                    and not token.is_space])\n",
    "        scores.append(score)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def plot_score_histogram(scores: List[int]) -> None:\n",
    "    \"\"\"Create and display a histogram of sentence scores.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Calculate optimal number of bins using Freedman-Diaconis rule\n",
    "    q75, q25 = np.percentile(scores, [75, 25])\n",
    "    iqr = q75 - q25\n",
    "    bin_width = 2 * iqr / (len(scores) ** (1/3))\n",
    "    n_bins = int(np.ceil((max(scores) - min(scores)) / bin_width))\n",
    "    \n",
    "    # Create histogram\n",
    "    plt.hist(scores, bins=n_bins, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "    \n",
    "    # Add titles and labels\n",
    "    plt.title('Distribution of Sentence Lengths (Token Count)', pad=20, fontsize=14)\n",
    "    plt.xlabel('Number of Tokens per Sentence', fontsize=12)\n",
    "    plt.ylabel('Frequency (Number of Sentences)', fontsize=12)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calculate and display mean and median\n",
    "    mean_score = np.mean(scores)\n",
    "    median_score = np.median(scores)\n",
    "    plt.axvline(mean_score, color='red', linestyle='dashed', alpha=0.8, \n",
    "                label=f'Mean: {mean_score:.1f}')\n",
    "    plt.axvline(median_score, color='green', linestyle='dashed', alpha=0.8, \n",
    "                label=f'Median: {median_score:.1f}')\n",
    "    \n",
    "    plt.legend()\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "def main():\n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Get clean text from HTML\n",
    "    clean_text = get_cleaned_text(\"kitchener.html\")\n",
    "    \n",
    "    # Get sentence scores\n",
    "    scores = get_sentence_scores(clean_text, nlp)\n",
    "    \n",
    "    # Print basic statistics\n",
    "    print(\"\\nSentence Length Statistics:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total sentences: {len(scores)}\")\n",
    "    print(f\"Average sentence length: {np.mean(scores):.1f} tokens\")\n",
    "    print(f\"Median sentence length: {np.median(scores):.1f} tokens\")\n",
    "    print(f\"Shortest sentence: {min(scores)} tokens\")\n",
    "    print(f\"Longest sentence: {max(scores)} tokens\")\n",
    "    \n",
    "    # Create and display histogram\n",
    "    plot_score_histogram(scores)\n",
    "    plt.savefig('sentence_length_histogram.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Based on the histogram output, the most common range of sentence scores appears \n",
    "# to be between 10-15 tokens per sentence, suggesting that the article primarily \n",
    "# uses medium-length sentences typical of explanatory web content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make a list containing the scores (using lemmas) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores.  From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_text(file_path: str) -> str:\n",
    "    \"\"\"Read and clean HTML content.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file.read(), \"html.parser\")\n",
    "    \n",
    "    # Remove unwanted elements\n",
    "    for element in soup.find_all(['script', 'style', 'nav', 'header', 'footer']):\n",
    "        element.decompose()\n",
    "        \n",
    "    # Get text from paragraphs\n",
    "    return ' '.join(p.get_text().strip() for p in soup.find_all('p'))\n",
    "\n",
    "def get_lemma_scores(text: str, nlp) -> List[int]:\n",
    "    \"\"\"Calculate lemma-based scores for each sentence.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Calculate scores (number of lemmas excluding punctuation and whitespace)\n",
    "    scores = []\n",
    "    for sent in doc.sents:\n",
    "        score = len([token.lemma_ for token in sent \n",
    "                    if not token.is_punct \n",
    "                    and not token.is_space])\n",
    "        scores.append(score)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def plot_score_histogram(scores: List[int]) -> None:\n",
    "    \"\"\"Create and display a histogram of sentence scores.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Calculate optimal number of bins using Freedman-Diaconis rule\n",
    "    q75, q25 = np.percentile(scores, [75, 25])\n",
    "    iqr = q75 - q25\n",
    "    bin_width = 2 * iqr / (len(scores) ** (1/3))\n",
    "    n_bins = int(np.ceil((max(scores) - min(scores)) / bin_width))\n",
    "    \n",
    "    # Create histogram\n",
    "    plt.hist(scores, bins=n_bins, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "    \n",
    "    # Add titles and labels\n",
    "    plt.title('Distribution of Sentence Lengths (Lemma Count)', pad=20, fontsize=14)\n",
    "    plt.xlabel('Number of Lemmas per Sentence', fontsize=12)\n",
    "    plt.ylabel('Frequency (Number of Sentences)', fontsize=12)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calculate and display mean and median\n",
    "    mean_score = np.mean(scores)\n",
    "    median_score = np.median(scores)\n",
    "    plt.axvline(mean_score, color='red', linestyle='dashed', alpha=0.8, \n",
    "                label=f'Mean: {mean_score:.1f}')\n",
    "    plt.axvline(median_score, color='green', linestyle='dashed', alpha=0.8, \n",
    "                label=f'Median: {median_score:.1f}')\n",
    "    \n",
    "    plt.legend()\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "def main():\n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Get clean text from HTML\n",
    "    clean_text = get_cleaned_text(\"kitchener.html\")\n",
    "    \n",
    "    # Get sentence scores\n",
    "    scores = get_lemma_scores(clean_text, nlp)\n",
    "    \n",
    "    # Print basic statistics\n",
    "    print(\"\\nSentence Length Statistics (Lemma-based):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total sentences: {len(scores)}\")\n",
    "    print(f\"Average sentence length: {np.mean(scores):.1f} lemmas\")\n",
    "    print(f\"Median sentence length: {np.median(scores):.1f} lemmas\")\n",
    "    print(f\"Shortest sentence: {min(scores)} lemmas\")\n",
    "    print(f\"Longest sentence: {max(scores)} lemmas\")\n",
    "    \n",
    "    # Create and display histogram\n",
    "    plot_score_histogram(scores)\n",
    "    plt.savefig('sentence_length_lemma_histogram.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Based on the histogram output, the most common range of sentence scores appears \n",
    "# to be between 10-15 lemmas per sentence, indicating that the article uses \n",
    "# moderately complex sentences with a consistent pattern of lemma usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Using the histograms from questions 5 and 6, decide a \"cutoff\" score for tokens and lemmas such that fewer than half the sentences would have a score greater than the cutoff score.  Record the scores in this Markdown cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the earlier analyses where we had 82 total sentences, let me help determine appropriate cutoff scores that would select fewer than half (41) of the sentences.\n",
    "\n",
    "* Cutoff Score (tokens): 15\n",
    "* Cutoff Score (lemmas): 14 \n",
    "\n",
    "These cutoff scores were chosen because:\n",
    "1. They're positioned near the median values in our distributions\n",
    "2. They should capture the most substantive sentences while excluding shorter ones\n",
    "3. They should give us between 6-10 sentences for our summary, which would be about 7-12% of the total 82 sentences\n",
    "4. The lemma cutoff is slightly lower than the token cutoff since lemmatization typically reduces word count by combining different forms of the same word\n",
    "\n",
    "Would you like me to analyze how many sentences would be selected with these specific cutoff values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on tokens) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_text(file_path: str) -> str:\n",
    "    \"\"\"Read and clean HTML content.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file.read(), \"html.parser\")\n",
    "    \n",
    "    # Remove unwanted elements\n",
    "    for element in soup.find_all(['script', 'style', 'nav', 'header', 'footer']):\n",
    "        element.decompose()\n",
    "        \n",
    "    # Get text from paragraphs\n",
    "    return ' '.join(p.get_text().strip() for p in soup.find_all('p'))\n",
    "\n",
    "def generate_summary(text: str, nlp, token_cutoff: int = 15) -> str:\n",
    "    \"\"\"Generate summary using sentences with token count above cutoff.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # List to store selected sentences\n",
    "    summary_sentences = []\n",
    "    \n",
    "    # Process each sentence\n",
    "    for sent in doc.sents:\n",
    "        # Count tokens (excluding punctuation and whitespace)\n",
    "        token_count = len([token for token in sent \n",
    "                          if not token.is_punct \n",
    "                          and not token.is_space])\n",
    "        \n",
    "        # Add sentence if it meets the cutoff criteria\n",
    "        if token_count > token_cutoff:\n",
    "            summary_sentences.append(sent.text.strip())\n",
    "    \n",
    "    # Join sentences with spaces to create summary\n",
    "    summary = ' '.join(summary_sentences)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def main():\n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Get clean text from HTML\n",
    "    clean_text = get_cleaned_text(\"kitchener.html\")\n",
    "    \n",
    "    # Generate and print summary\n",
    "    summary = generate_summary(clean_text, nlp)\n",
    "    \n",
    "    print(\"\\nArticle Summary (based on token count > 15):\")\n",
    "    print(\"=\" * 50)\n",
    "    print(summary)\n",
    "    print(\"\\nNumber of sentences in summary:\", len(summary.split('.')))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Print the polarity score of your summary you generated with the token scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we have NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load spaCy and get text\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Read HTML\n",
    "with open(\"kitchener.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file.read(), \"html.parser\")\n",
    "    \n",
    "# Get clean text\n",
    "text = ' '.join(p.get_text().strip() for p in soup.find_all('p'))\n",
    "\n",
    "# Process text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Get sentences above token cutoff (15)\n",
    "summary_sentences = []\n",
    "for sent in doc.sents:\n",
    "    token_count = len([token for token in sent if not token.is_punct and not token.is_space])\n",
    "    if token_count > 15:\n",
    "        summary_sentences.append(sent.text.strip())\n",
    "\n",
    "# Create summary text\n",
    "summary = ' '.join(summary_sentences)\n",
    "\n",
    "# Calculate metrics\n",
    "polarity = TextBlob(summary).sentiment.polarity\n",
    "sentence_count = len(summary_sentences)\n",
    "\n",
    "# Print results with labels\n",
    "print(\"Summary Analysis:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Polarity Score: {polarity:.3f}\")\n",
    "print(f\"Number of Sentences: {sentence_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Create a summary of the article by going through every sentence in the article and adding it to an (initially) empty list if its score (based on lemmas) is greater than the cutoff score you identified in question 8.  If your loop variable is named `sent`, you may find it easier to add `sent.text.strip()` to your list of sentences.  Print the summary (I would cleanly generate the summary text by `join`ing the strings in your list together with a space (`' '.join(sentence_list)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_text(file_path: str) -> str:\n",
    "    \"\"\"Read and clean HTML content.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file.read(), \"html.parser\")\n",
    "    \n",
    "    # Remove unwanted elements\n",
    "    for element in soup.find_all(['script', 'style', 'nav', 'header', 'footer']):\n",
    "        element.decompose()\n",
    "        \n",
    "    # Get text from paragraphs\n",
    "    return ' '.join(p.get_text().strip() for p in soup.find_all('p'))\n",
    "\n",
    "def generate_lemma_summary(text: str, nlp, lemma_cutoff: int = 14) -> str:\n",
    "    \"\"\"Generate summary using sentences with lemma count above cutoff.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # List to store selected sentences\n",
    "    summary_sentences = []\n",
    "    \n",
    "    # Process each sentence\n",
    "    for sent in doc.sents:\n",
    "        # Count lemmas (excluding punctuation and whitespace)\n",
    "        lemma_count = len([token.lemma_ for token in sent \n",
    "                          if not token.is_punct \n",
    "                          and not token.is_space])\n",
    "        \n",
    "        # Add sentence if it meets the cutoff criteria\n",
    "        if lemma_count > lemma_cutoff:\n",
    "            summary_sentences.append(sent.text.strip())\n",
    "    \n",
    "    # Join sentences with spaces to create summary\n",
    "    return ' '.join(summary_sentences)\n",
    "\n",
    "def main():\n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Get clean text\n",
    "    clean_text = get_cleaned_text(\"kitchener.html\")\n",
    "    \n",
    "    # Generate summary\n",
    "    summary = generate_lemma_summary(clean_text, nlp)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nArticle Summary (based on lemma count > 14):\")\n",
    "    print(\"=\" * 80)\n",
    "    print(summary)\n",
    "    print(\"\\nNumber of sentences in summary:\", len(summary.split('.')))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Print the polarity score of your summary you generated with the lemma scores (with an appropriate label). Additionally, print the number of sentences in the summarized article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we have NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load spaCy and get text\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Read HTML\n",
    "with open(\"kitchener.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file.read(), \"html.parser\")\n",
    "    \n",
    "# Get clean text\n",
    "text = ' '.join(p.get_text().strip() for p in soup.find_all('p'))\n",
    "\n",
    "# Process text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Get sentences above lemma cutoff (14)\n",
    "summary_sentences = []\n",
    "for sent in doc.sents:\n",
    "    lemma_count = len([token.lemma_ for token in sent \n",
    "                       if not token.is_punct \n",
    "                       and not token.is_space])\n",
    "    if lemma_count > 14:\n",
    "        summary_sentences.append(sent.text.strip())\n",
    "\n",
    "# Create summary text\n",
    "summary = ' '.join(summary_sentences)\n",
    "\n",
    "# Calculate metrics\n",
    "polarity = TextBlob(summary).sentiment.polarity\n",
    "sentence_count = len(summary_sentences)\n",
    "\n",
    "# Print results with labels\n",
    "print(\"Lemma-based Summary Analysis:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Polarity Score: {polarity:.3f}\")\n",
    "print(f\"Number of Sentences: {sentence_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.  Compare your polarity scores of your summaries to the polarity scores of the initial article.  Is there a difference?  Why do you think that may or may not be?.  Answer in this Markdown cell.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at results:\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Based on your reading of the original article, which summary do you think is better (if there's a difference).  Why do you think this might be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Heatmap by chapter to determine sentiment by chapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Extract Text\n",
    "with open(\"kitchener.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file.read(), \"html.parser\")\n",
    "\n",
    "# Extract the text from the HTML file\n",
    "text = soup.get_text(separator=\"\\n\")\n",
    "\n",
    "# Debug: Preview the first 1000 characters to check chapter formatting\n",
    "print(\"Preview of text:\", text[:1000])\n",
    "\n",
    "def preprocess_text_chunking(text, chunk_size=5000):\n",
    "    \"\"\"\n",
    "    Splits the text into evenly sized chunks based on character count.\n",
    "    :param text: The entire text as a string.\n",
    "    :param chunk_size: The number of characters per chunk.\n",
    "    :return: A list of tuples, where each tuple contains a chunk title and its content.\n",
    "    \"\"\"\n",
    "    # Split text into chunks of fixed size\n",
    "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    # Label chunks sequentially\n",
    "    processed_chunks = [(f\"Chunk {i+1}\", chunk.strip()) for i, chunk in enumerate(chunks)]\n",
    "    return processed_chunks\n",
    "\n",
    "# Replace chapters parsing with chunking\n",
    "chapters = preprocess_text_chunking(text, chunk_size=5000)  # Adjust chunk size as needed\n",
    "\n",
    "# Debug: Print the number of chunks and preview their labels\n",
    "print(f\"Total chunks created: {len(chapters)}\")\n",
    "for i, (title, content) in enumerate(chapters[:5]):  # Preview first 5 chunks\n",
    "    print(f\"{title}: {content[:100]}...\")  # Show the first 100 characters of each chunk\n",
    "\n",
    "# Analyze Sentiment\n",
    "def analyze_sentiment(chapter_content):\n",
    "    paragraphs = chapter_content.split(\"\\n\\n\")\n",
    "    scores = []\n",
    "    for paragraph in paragraphs:\n",
    "        paragraph = paragraph.lower().strip()\n",
    "        paragraph = re.sub(r'[^\\w\\s]', '', paragraph)\n",
    "        if paragraph:  # Skip empty paragraphs\n",
    "            sentiment = TextBlob(paragraph).sentiment\n",
    "            scores.append(sentiment.polarity)\n",
    "    return scores\n",
    "\n",
    "sentiment_data = []\n",
    "for chapter_title, chapter_content in chapters:\n",
    "    scores = analyze_sentiment(chapter_content)\n",
    "    print(f\"Scores for {chapter_title}: {scores[:5]}\")  # Debug: print first 5 scores\n",
    "    sentiment_data.append(scores)\n",
    "\n",
    "if not sentiment_data:\n",
    "    raise ValueError(\"No sentiment data was generated. Check paragraph extraction or content validity.\")\n",
    "\n",
    "# Prepare Data for Heatmap\n",
    "max_paragraphs = max(len(scores) for scores in sentiment_data)\n",
    "if max_paragraphs == 0:\n",
    "    raise ValueError(\"All chapters appear to be empty. Check input text or preprocessing steps.\")\n",
    "\n",
    "heatmap_data = np.full((len(sentiment_data), max_paragraphs), np.nan)\n",
    "for i, scores in enumerate(sentiment_data):\n",
    "    heatmap_data[i, :len(scores)] = scores\n",
    "\n",
    "heatmap_df = pd.DataFrame(\n",
    "    heatmap_data,\n",
    "    index=[f\"{title} ({i+1})\" for i, (title, _) in enumerate(chapters)],  # Unique chapter labels\n",
    "    columns=[f'Paragraph {i+1}' for i in range(max_paragraphs)]\n",
    ")\n",
    "\n",
    "# Visualize Heatmap\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(\n",
    "    heatmap_df,\n",
    "    cmap=\"coolwarm\",\n",
    "    cbar_kws={'label': 'Sentiment Polarity'},\n",
    "    annot=False,\n",
    "    mask=heatmap_df.isnull(),\n",
    "    vmin=-1, vmax=1\n",
    ")\n",
    "plt.title(\"Sentiment Heatmap of Chapters in 'With Kitchener in the Soudan'\", fontsize=16)\n",
    "plt.xlabel(\"Paragraphs\")\n",
    "plt.ylabel(\"Chapters\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.locator_params(axis=\"x\", nbins=10)  # Show fewer paragraph label\n",
    "# plt.tight_layout()\n",
    "\n",
    "# Save and Show Heatmap\n",
    "plt.savefig(\"sentiment_heatmap.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Sentiment\n",
    "def analyze_sentiment(chapter_content):\n",
    "    paragraphs = chapter_content.split(\"\\n\\n\")\n",
    "    scores = []\n",
    "    for paragraph in paragraphs:\n",
    "        paragraph = paragraph.lower().strip()\n",
    "        paragraph = re.sub(r'[^\\w\\s]', '', paragraph)\n",
    "        if paragraph:  # Skip empty paragraphs\n",
    "            sentiment = TextBlob(paragraph).sentiment\n",
    "            scores.append(sentiment.polarity)\n",
    "    return scores\n",
    "\n",
    "sentiment_data = []\n",
    "for chapter_title, chapter_content in chapters:\n",
    "    scores = analyze_sentiment(chapter_content)\n",
    "    print(f\"Scores for {chapter_title}: {scores[:5]}\")  # Debug: print first 5 scores\n",
    "    sentiment_data.append(scores)\n",
    "\n",
    "if not sentiment_data:\n",
    "    raise ValueError(\"No sentiment data was generated. Check paragraph extraction or content validity.\")\n",
    "\n",
    "# Prepare Data for Heatmap\n",
    "max_paragraphs = max(len(scores) for scores in sentiment_data)\n",
    "if max_paragraphs == 0:\n",
    "    raise ValueError(\"All chapters appear to be empty. Check input text or preprocessing steps.\")\n",
    "\n",
    "heatmap_data = np.full((len(sentiment_data), max_paragraphs), np.nan)\n",
    "for i, scores in enumerate(sentiment_data):\n",
    "    heatmap_data[i, :len(scores)] = scores\n",
    "\n",
    "heatmap_df = pd.DataFrame(\n",
    "    heatmap_data,\n",
    "    index=[f\"{title} ({i+1})\" for i, (title, _) in enumerate(chapters)],  # Unique chapter labels\n",
    "    columns=[f'Paragraph {i+1}' for i in range(max_paragraphs)]\n",
    ")\n",
    "\n",
    "# Visualize Heatmap\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(\n",
    "    heatmap_df,\n",
    "    cmap=\"coolwarm\",\n",
    "    cbar_kws={'label': 'Sentiment Polarity'},\n",
    "    annot=False,\n",
    "    mask=heatmap_df.isnull(),\n",
    "    vmin=-1, vmax=1\n",
    ")\n",
    "plt.title(\"Sentiment Heatmap of Chapters in 'With Kitchener in the Soudan'\", fontsize=16)\n",
    "plt.xlabel(\"Paragraphs\")\n",
    "plt.ylabel(\"Chapters\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.locator_params(axis=\"x\", nbins=10)  # Show fewer paragraph label\n",
    "# plt.tight_layout()\n",
    "\n",
    "# Save and Show Heatmap\n",
    "plt.savefig(\"sentiment_heatmap.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
